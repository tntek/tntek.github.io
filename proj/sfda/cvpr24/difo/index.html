<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>title</title>
    <link rel="stylesheet" href="./style..css">
</head>
<body>
    <div class="container">
        <div class="title">
            <h2>Source-Free Domain Adaptation with Frozen Multimodal Foundation Model</h2>
            <p>CVPR2024</p>
            <div class="nav">
                <a href="#">Song Tang<sup>1,2,3</sup></a>
                <a href="#">Wenxin Su<sup>1</sup></a>
                <a href="#">Mao Ye<sup>*4</sup></a>
                <a href="#">Xiatian Zhu<sup>*5</sup></a>
            </div>
            <div class="jieshao">
                <p><sup>1</sup>University of Shanghai for Science and Technology <sup>2</sup>Universit¨at Hamburg <sup>3</sup>ComOriginMat Inc <sup>4</sup>University of Electronic Science and Technology of China
             <sup>5</sup>University of Surrey</p>
            </div>
        </div>
        <div class="menu">
            <a href="https://arxiv.org/pdf/2311.16510">Paper</a>
            <a href="https://github.com/tntek/tntek.github.io/blob/main/proj/sfda/cvpr24/difo/file/10888_supp.pdf">Supplementary</a>
            <a href="poster.html">Poster</a>
            <a href="https://github.com/tntek/source-free-domain-adaptation">PyTorch Code</a>
        </div>
        <div id="posterImage" style="display:none; text-align:center;">
            <img src="https://github.com/tntek/tntek.github.io/raw/main/proj/sfda/cvpr24/difo/file/poster.png" alt="Poster Image" style="max-width:100%; height:auto;">
        </div>
        <div class="title">
            <p>Abstract</p>
            <div class="Abstract">
                <p>Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a target domain, with only access to 
unlabeled target training data and the source model pretrained on a supervised source domain. Relying on pseudo 
labeling and/or auxiliary supervision, conventional methods are inevitably error-prone. To mitigate this limitation, 
in this work we for the first time explore the potentials of off-the-shelf vision-language (ViL) multimodal models (e.g.,
CLIP) with rich whilst heterogeneous knowledge. We find that directly applying the ViL model to the target domain in a 
zero-shot fashion is unsatisfactory, as it is not specialized for this particular task but largely generic. To make it task spe- 
cific, we propose a novel Distilling multImodal Foundation model (DIFO) approach. Specifically, DIFO alternates between two steps during adaptation: (i) Customizing the 
ViL model by maximizing the mutual information with the target model in a prompt learning manner, (ii) Distilling 
the knowledge of this customized ViL model to the target model. For more fine-grained and reliable distillation, we 
further introduce two effective regularization terms, namely most-likely category encouragement and predictive consis-
tency. Extensive experiments show that DIFO significantly outperforms the state-of-the-art alternatives. Code is here.</p>
    <div class="content">
        <h1 class="title">Overview of DIFO</h1>
        <div class="image-container">
            <img src="file/method.png" alt="Description of the image">
            <p class="image-description">
                The process involves two alternating steps. First, we perform (a) task-specific customization of a ViL model through task-specific prompt learning (LTsc).
                This is achieved under soft predictive guidance using mutual information maximization. Second, we undertake (b) memory-aware knowledge adaptation,
                incorporating two regularizations: most-likely category encouragement (LMCE) predicted by our dynamic memory-aware predictor,
                along with the typical predictive consistency (LPC). These regularizations are designed to facilitate a coarse-to-fine adaptation.
            </p>
            <h1 class="title">Result</h1>
        <p class="image-description">Closed-set SFDA on Office-Home and VisDA (%). SF and M means source-free and multimodal, respectively; the full results on VisDA are in Supplementary.</p>
        <img src="file/table2.png" alt="Description of the image"> 
        <p class="image-description">Closed-set SFDA on DomainNet-126 (%). SF and M means source-free and multimodal, respectively.</p>
        <img src="file/table3.png" alt="Description of the additional results">
        <p class="image-description"> Results (%) of CLIP and Source+CLIP on the four evaluation datasets. The backbone of CLIP image-encoder in CLP-C-RN and
CLP-C-B32 are the same as DIFO-C-RN and DIFO-C-B32, respectively. The full results are provided in Supplementary</p>
       <img src="file/table4.png" alt="Description of the additional results">
        <p class="image-description"> Feature distribution visualization comparison on transfer task Ar→Cl in Office-Home. Oracle is trained on target domain Cl using
the ground-truth labels. Different colors stand for different categories. 
       <img src="file/table7.png" alt="Description of the additional results">
        </div>
    </div>
</div>
        
        
        <div class="citation">
            <p>Citation</p>
            <span>If you find our work helpful in your research, please cite our work:</span>
            <div>
                @InProceedings{Song Tang_2024_CVPR,<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Source-Free Domain Adaptation with Frozen Multimodal Foundation Model},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Song Tang, Wenxin Su, Mao Ye, Xiatian Zhu},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;month={November},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2024},<br>
<!--                     &nbsp;&nbsp;&nbsp;&nbsp;pages={7046-7056}<br> -->
                }
            </div>
            <p>License</p>
            <span>This project is licenced under an<a style="color: rgb(30, 108, 147);font-weight: bold;" href="#">[MIT License]</a>.</span>
            <p>Contact</p>
            <span>If you have any queries, please get in touch via email : <a style="color: rgb(30, 108, 147);font-weight: bold;" href="#"> suwenxin43@gmail.com</a></span>
        </div>
    </div>
</body>
</html>
